Using the randomly generated datasets, the goal is to analyze and understand user behavior and app performance. Below are some insights and their corresponding PySpark implementations:

Daily Active Users - There are 634485 Users who have maintained a daily streak for the year 2024. The calculation is calculated as the person logging on consecutive days then it is counted as 1 else 0 and finally it is summed up.
Monthly Active Users - There are 0 Users who have logged in every month for the year 2024.
Session-Based Metrics: The highest average session was close to 30 minutes with 2 actions performed during the session.

Furthermore, analysis like User retention, most popular actions, subscription insights, activity trends by time of day, app version analysis, outlier detection -  low/high duration_ms values, etc can be done.

Additionally, I noticed that reading/writing from/to a columnar file format like Parquet and ORC is efficient compared to CSV and other row-based file formats. 
The main reason is that the data is stored column-wise which makes it easier to fetch and usually big data is partitioned so partition pruning is done while reading. 
Also, data in parquet and ORC is compressed which benefits us in storage costs. Computation costs can be indirectly achieved as data is read and processed faster.
